We Read 24 papers

Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We select and analyze 395 research papers from January 2017 to January 2024 to answer four key research questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, preprocessing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and flagging promising areas for future study.

This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.

Software developers often repeat the same code changes within a project or across different projects. These repetitive changes are known as “code change patterns” (CPATs). Automating CPATs is crucial to expedite the software development process. While current Transformation by Example (TBE) techniques can automate CPATs, they are limited by the quality and quantity of the provided input examples. Thus, they miss transforming code variations that do not have the exact syntax, data-, or control-flow of the provided input examples, despite being semantically similar. Large Language Models (LLMs), pre-trained on extensive source code datasets, offer a potential solution. Harnessing the capability of LLMs to generate semantically equivalent, yet previously unseen variants of the original CPAT could significantly increase the effectiveness of TBE systems. In this paper, we first discover best practices for harnessing LLMs to generate code variants that meet three criteria: correctness (semantic equivalence to the original CPAT), usefulness (reflecting what developers typically write), and applicability (aligning with the primary intent of the original CPAT). We then implement these practices in our tool PyCraft, which synergistically combines static code analysis, dynamic analysis, and LLM capabilities. By employing chain-of-thought reasoning, PyCraft generates variations of input examples and comprehensive test cases that identify correct variations with an F-measure of 96.6%. Our algorithm uses feedback iteration to expand the original input examples by an average factor of 58x. Using these richly generated examples, we inferred transformation rules and then automated these changes, resulting in an increase of up to 39x, with an average increase of 14x in target codes compared to a previous state-of-the-art tool that relies solely on static analysis. We submitted patches generated by PyCraft to a range of projects, notably esteemed ones like microsoft/DeepSpeed and IBM/inFairness. Their developers accepted and merged 83% the 86 CPAT instances submitted through 44 pull requests. This confirms the usefulness of these changes.

Long methods that encapsulate multiple responsibilities within a single method are challenging to maintain. Choosing which statements to extract into new methods has been the target of many research tools. Despite steady improvements, these tools often fail to generate refactorings that align with developers' preferences and acceptance criteria. Given that Large Language Models (LLMs) have been trained on large code corpora, if we harness their familiarity with the way developers form functions, we could suggest refactorings that developers are likely to accept. In this paper, we advance the science and practice of refactoring by synergistically combining the insights of LLMs with the power of IDEs to perform Extract Method (EM). Our formative study on 1752 EM scenarios revealed that LLMs are very effective for giving expert suggestions, yet they are unreliable: up to 76.3% of the suggestions are hallucinations. We designed a novel approach that removes hallucinations from the candidates suggested by LLMs, then further enhances and ranks suggestions based on static analysis techniques from program slicing, and finally leverages the IDE to execute refactorings correctly. We implemented this approach in an IntelliJ IDEA plugin called EM-Assist. We empirically evaluated EM-Assist on a diverse corpus that replicates 1752 actual refactorings from open-source projects. We found that EM-Assist outperforms previous state of the art tools: EM-Assist suggests the developer-performed refactoring in 53.4% of cases, improving over the recall rate of 39.4% for previous best-in-class tools. Furthermore, we conducted firehouse surveys with 16 industrial developers and suggested refactorings on their recent commits. 81.3% of them agreed with the recommendations provided by EM-Assist. This shows the usefulness of our approach and ushers us into a new era when LLMs become effective AI assistants for refactoring.

MOVEMETHOD is a hallmark refactoring. Despite
a plethora of research tools that recommend which methods to
move and where, these recommendations do not align with how
expert developers perform MOVEMETHOD. Given the extensive
training of Large Language Models and their reliance upon nat-
uralness of code, they should expertly recommend which methods
are misplaced in a given class and which classes are better hosts.
Our formative study of 2016 LLM recommendations revealed
that LLMs give expert suggestions, yet they are unreliable: up
to 80% of the suggestions are hallucinations.
We introduce the first LLM fully powered assistant for
MOVEMETHOD refactoring that automates its whole end-to-end
lifecycle, from recommendation to execution. We designed novel
solutions that automatically filter LLM hallucinations using static
analysis from IDEs and a novel workflow that requires LLMs
to be self-consistent, critique, and rank refactoring suggestions.
As MOVEMETHOD refactoring requires global, project-level
reasoning, we solved the limited context size of LLMs by employ-
ing refactoring-aware retrieval augment generation (RAG). Our
approach, MM-ASSIST, synergistically combines the strengths of
the LLM, IDE, static analysis, and semantic relevance. In our
thorough, multi-methodology empirical evaluation, we compare
MM-ASSIST with the previous state-of-the-art approaches. MM-
ASSIST significantly outperforms them: (i) on a benchmark
widely used by other researchers, our Recall@1 and Recall@3
show a 1.7x improvement; (ii) on a corpus of 210 recent
refactorings from Open-source software, our Recall rates improve
by at least 2.4x. Lastly, we conducted a user study with 30
experienced participants who used MM-ASSIST to refactor their
own code for one week. They rated 82.8% of MM-ASSIST
recommendations positively. This shows that MM-ASSIST is both
effective and useful.

What role could AI — especially large language models (LLMs) — play in the work of maintaining software systems? This keynote examines how we can boost the reasoning capabilities of LLMs across three essential maintenance activities: coding, critiquing, and curing software. The first part focuses on code, exploring how we can improve code generation by distilling and utilizing structured reasoning traces inspired by software development practices. The second part addresses critique, illustrating how AI can reason about software vulnerabilities by learning from contrastive reasoning pairs that distinguish valid and flawed explanations. The third part turns to cure, highlighting recent advances that expand the search for fixes using diverse inputs and multi-stage reasoning signals. Across these tasks, we show how advancing LLM reasoning requires data-centric innovations — enriching, linking, and transforming software artefacts to better support structured, task-specific reasoning. The talk concludes with a brief reflection on the road ahead and open questions for future exploration.

Developers often evolve an existing software system by making
internal changes, called migration. Moving to a new framework,
changing implementation to improve efficiency, and upgrading a
dependency to its latest version are examples of migrations.
Migration is a common and typically continuous maintenance
task undertaken either manually or through tooling. Certain mi-
grations are labor intensive and costly, developers do not find the
required work rewarding, and they may take years to complete.
Hence, automation is preferred for such migrations.
In this paper, we discuss a large-scale, costly and traditionally
manual migration project at Google, propose a novel automated
algorithm that uses change location discovery and a Large Language
Model (LLM) to aid developers conduct the migration, report the
results of a large case study, and discuss lessons learned.
Our case study on 39 distinct migrations undertaken by three de-
velopers over twelve months shows that a total of 595 code changes
with 93, 574 edits have been submitted, where 74.45% of the code
changes and 69.46% of the edits were generated by the LLM. The de-
velopers reported high satisfaction with the automated tooling, and
estimated a 50% reduction on the total time spent on the migration
compared to earlier manual migrations.
Our results suggest that our automated, LLM-assisted workflow
can serve as a model for similar initiatives.

Writing software tests is laborious and time-consuming. To address this, prior studies introduced various automated test-generation techniques. A well-explored research direction in this field is unit test generation, wherein artificial intelligence (AI) techniques create tests for a method/class under test. While many of these techniques have primarily found applications in a research context, existing tools (e.g., EvoSuite, Randoop, and AthenaTest) are not user-friendly and are tailored to a single technique. This paper introduces TestSpark, a plugin for IntelliJ IDEA that enables users to generate unit tests with only a few clicks directly within their Integrated Development Environment (IDE). Furthermore, TestSpark also allows users to easily modify and run each generated test and integrate them into the project workflow. TestSpark leverages the advances of search-based test generation tools, and it introduces a technique to generate unit tests using Large Language Models (LLMs) by creating a feedback cycle between the IDE and the LLM. Since TestSpark is an open-source (this https URL), extendable, and well-documented tool, it is possible to add new test generation methods into the plugin with the minimum effort. This paper also explains our future studies related to TestSpark and our preliminary results. 

Large Language Models (LLMs) are widely adopted for automated code generation with promising results. Although prior research has assessed LLM-generated code and identified various quality issues -- such as redundancy, poor maintainability, and sub-optimal performance a systematic understanding and categorization of these inefficiencies remain unexplored. Without such knowledge, practitioners struggle to optimize LLM-generated code for real-world applications, limiting its adoption. This study can also guide improving code LLMs, enhancing the quality and efficiency of code generation. Therefore, in this study, we empirically investigate inefficiencies in LLM-generated code by state-of-the-art models, i.e., CodeLlama, DeepSeek-Coder, and CodeGemma. To do so, we analyze 492 generated code snippets in the HumanEval++ dataset. We then construct a taxonomy of inefficiencies in LLM-generated code that includes 5 categories General Logic, Performance, Readability, Maintainability, and Errors) and 19 subcategories of inefficiencies. We then validate the proposed taxonomy through an online survey with 58 LLM practitioners and researchers. Our study indicates that logic and performance-related inefficiencies are the most popular, relevant, and frequently co-occur and impact overall code quality inefficiency. Our taxonomy provides a structured basis for evaluating the quality LLM-generated code and guiding future research to improve code generation efficiency.

Human-written API documentation often becomes
outdated, requiring developers to update it manually. Researchers
have proposed identifying outdated API references in documen-
tation, yet have not addressed updating API documentation. Now,
emerging large language models (LLMs) are capable of gener-
ating code examples and text descriptions. Then, a key question
arises: Can LLMs assist in updating API documentation? In
this paper, we propose an approach for leveraging an LLM
to update API documentation with code change information.
To evaluate this approach, we select five open-source projects
that manage documentation revisions on GitHub and analyze
the differences in documentation between two releases to derive
ground truths. We then assess the accuracy of LLM-generated
updates by comparing them to the ground truths. Our results
show that LLM-generated updates achieve higher METEOR than
outdated API documentation (0.771 vs 0.679). It indicates that
the LLM updates are more similar to the human updates than
the outdated documentation. Our results also reveal that LLMs
update code-related information in API documentation with a
maximum F1 score of 0.921

Researchers have made significant progress in automating the software development process in the past decades. Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding. Nevertheless, software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. bug fixing) and software evolution (e.g. feature additions). In this paper, we propose an automated approach for solving GitHub issues to autonomously achieve program improvement. In our approach called AutoCodeRover, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented. We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. Our code search exploits the program structure in the form of classes/methods to enhance LLM's understanding of the issue's root cause, and effectively retrieve a context via iterative search. The use of spectrum-based fault localization using tests, further sharpens the context, as long as a test-suite is available. Experiments on SWE-bench-lite (300 real-life GitHub issues) show increased efficacy in solving GitHub issues (19% on SWE-bench-lite), which is higher than the efficacy of the recently reported SWE-agent. In addition, AutoCodeRover achieved this efficacy with significantly lower cost (on average, $0.43 USD), compared to other baselines. We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved.

Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.

The last several years saw the emergence of AI assistants for
code — multi-purpose AI-based helpers in software engineering. As they be-
come omnipresent in all aspects of software development, it becomes critical
to understand their usage patterns.
Objective. We aim to better understand how specifically developers are
using AI assistants, why they are not using them in certain parts of their
development workflow, and what needs to be improved in the future.
Methods. In this work, we carried out a large-scale survey aimed at
how AI assistants are used, focusing on specific software development activ-
ities and stages. We collected opinions of 481 programmers on five broad
activities: (a) implementing new features, (b) writing tests, (c) bug triaging,
(d) refactoring, and (e) writing natural-language artifacts, as well as their
individual stages.
Results. Our results provide a novel comparison of different stages where
AI assistants are used that is both comprehensive and detailed. It highlights
specific activities that developers find less enjoyable and want to delegate
to an AI assistant, e.g., writing tests and natural-language artifacts. We
also determine more granular stages where AI assistants are used, such as
generating tests and generating docstrings, as well as less studied parts of
the workflow, such as generating test data. Among the reasons for not using
assistants, there are general aspects like trust and company policies, as well
as more concrete issues like the lack of project-size context, which can be the
focus of the future research.
Conclusion. The provided analysis highlights stages of software devel-
opment that developers want to delegate and that are already popular for
using AI assistants, which can be a good focus for features aimed to help
developers right now. The main reasons for not using AI assistants can serve
as a guideline for future work.

Automated Program Repair (APR) uses various tools
and techniques to help developers achieve functional and error-
free code faster. In recent years, Large Language Models (LLMs)
have gained popularity as components in APR tool chains because
of their performance and flexibility. However, training such
models requires a significant amount of resources. Fine-tuning
techniques have been developed to adapt pre-trained LLMs to
specific tasks, such as APR, and enhance their performance at
far lower computational costs than training from scratch.
In this study, we empirically investigate the impact of various
fine-tuning techniques on the performance of LLMs used for
APR. Our experiments provide insights into the performance
of a selection of state-of-the-art LLMs pre-trained on code.
The evaluation is done on three popular APR benchmarks (i.e.,
QuixBugs, Defects4J and HumanEval-Java) and considers six
different LLMs with varying parameter sizes (resp. CodeGen,
CodeT5, StarCoder, DeepSeekCoder, Bloom, and CodeLlama-
2). We consider three training regimens: no fine-tuning, full
fine-tuning, and parameter-efficient fine-tuning (PEFT) using
LoRA and IA3. We observe that full fine-tuning techniques
decrease the benchmarking performance of various models due to
different data distributions and overfitting. By using parameter-
efficient fine-tuning methods, we restrict models in the amount
of trainable parameters and achieve better results

Software Engineering Agents (SWE agents) can
autonomously perform development tasks on benchmarks like
SWE Bench, but still face challenges when tackling complex
and ambiguous real-world tasks. Consequently, SWE agents are
often designed to allow interactivity with developers, enabling
collaborative problem-solving. To understand how developers
collaborate with SWE agents and the barriers they face in
such interactions, we observed 19 developers using an in-IDE
agent to resolve 33 open issues in repositories to which they
had previously contributed. Participants successfully resolved
about half of these issues, with those solving issues incrementally
having greater success than those using a one-shot approach.
Participants who actively collaborated with the agent and iterated
on its outputs were also more successful, though they faced
challenges in trusting the agent’s responses and collaborating
on debugging and testing. Our findings suggest that to facili-
tate successful collaborations, both SWE agents and developers
should actively contribute to tasks throughout all stages of the
software development process. SWE agents can enable this by
challenging and engaging in discussions with developers, rather
than being conclusive or sycophantic.

Software refactoring is an essential activity for improving the readability, maintainability, and reusability of software projects. To
this end, a large number of automated or semi-automated approaches/tools have been proposed to locate poorly designed code,
recommend refactoring solutions, and conduct specified refactorings. However, even equipped with such tools, it remains challenging
for developers to decide where and what kind of refactorings should be applied. Recent advances in deep learning techniques, especially
in large language models (LLMs), make it potentially feasible to automatically refactor source code with LLMs. However, it remains
unclear how well LLMs perform compared to human experts in conducting refactorings automatically and accurately. To fill this
gap, in this paper, we conduct an empirical study to investigate the potential of LLMs in automated software refactoring, focusing on
the identification of refactoring opportunities and the recommendation of refactoring solutions. We first construct a high-quality
refactoring dataset comprising 180 real-world refactorings from 20 projects, and conduct the empirical study on the dataset. With
the to-be-refactored Java documents as input, ChatGPT and Gemini identified only 28 and 7 respectively out of the 180 refactoring
opportunities. However, explaining the expected refactoring subcategories and narrowing the search space in the prompts substantially
increased the success rate of ChatGPT from 15.6% to 86.7%. Concerning the recommendation of refactoring solutions, ChatGPT
recommended 176 refactoring solutions for the 180 refactorings, and 63.6% of the recommended solutions were comparable to (even
better than) those constructed by human experts. However, 13 out of the 176 solutions suggested by ChatGPT and 9 out of the 137
solutions suggested by Gemini were unsafe in that they either changed the functionality of the source code or introduced syntax errors,
which indicate the risk of LLM-based refactoring. To this end, we propose a detect-and-reapply tactic, called RefactoringMirror,
to avoid such unsafe refactorings. By reapplying the identified refactorings to the original code using thoroughly tested refactoring
engines, we can effectively mitigate the risks associated with LLM-based automated refactoring while still leveraging LLM’s intelligence
to obtain valuable refactoring recommendations. Our evaluation results suggest that RefactoringMirror accurately identified and
reapplied 94.3% of the refactorings conducted by LLMs, and successfully avoided all of the buggy solutions.

Refactoring detection is crucial for a variety of applications and tasks: (i) empirical studies about code evolution, (ii) tools for
library API migration, (iii) code reviews and change comprehension. However, recent research has questioned the accuracy of the
state-of-the-art refactoring mining tools, which poses threats to the reliability of the detected refactorings. Moreover, the majority of
refactoring mining tools depend on code similarity thresholds. Finding universal threshold values that can work well for all projects,
regardless of their architectural style, application domain, and development practices is extremely challenging. Therefore, in a previous
work [1], we introduced the first refactoring mining tool that does not require any code similarity thresholds to operate. In this work, we
extend our tool to support low-level refactorings that take place within the body of methods. To evaluate our tool, we created one of the
most accurate, complete, and representative refactoring oracles to date, including 7,226 true instances for 40 different refactoring types
detected by one (minimum) up to six (maximum) different tools, and validated by one up to four refactoring experts. Our evaluation
showed that our approach achieves the highest average precision (99.6%) and recall (94%) among all competitive tools, and on
median is 2.6 times faster than the second faster competitive tool.

Effort estimation is a crucial activity in agile soft-
ware development, where teams collaboratively review, discuss,
and estimate the effort required to complete user stories in a
product backlog. Current practices in agile effort estimation
heavily rely on subjective assessments, leading to inaccuracies and
inconsistencies in the estimates. While recent machine learning-
based methods show promising accuracy, they cannot explain or
justify their estimates and lack the capability to interact with
human team members. Our paper fills this significant gap by
leveraging the powerful capabilities of Large Language Models
(LLMs). We propose a novel LLM-based multi-agent framework
for agile estimation that not only can produce estimates, but also
can coordinate, communicate and discuss with human developers
and other agents to reach a consensus. Evaluation results on a
real-life dataset show that our approach outperforms state-of-the-
art techniques across all evaluation metrics in the majority of the
cases. Our human study with software development practitioners
also demonstrates an overwhelmingly positive experience in
collaborating with our agents in agile effort estimation.

Maintaining and scaling software systems relies heavily on effective
code refactoring, yet this process remains labor-intensive, requir-
ing developers to carefully analyze existing codebases and prevent
the introduction of new defects. Although recent advancements
have leveraged Large Language Models (LLMs) to automate refac-
toring tasks, current solutions are constrained in scope and lack
mechanisms to guarantee code compilability and successful test
execution. In this work, we introduce MANTRA, a comprehensive
LLM agent-based framework that automates method-level refac-
toring. MANTRA integrates Context-Aware Retrieval-Augmented
Generation, coordinated Multi-Agent Collaboration, and Verbal
Reinforcement Learning to emulate human decision-making dur-
ing refactoring while preserving code correctness and readability.
Our empirical study, conducted on 703 instances of “pure refactor-
ings” (i.e., code changes exclusively involving structural improve-
ments), drawn from 10 representative Java projects, covers the
six most prevalent refactoring operations. Experimental results
demonstrate that MANTRA substantially surpasses a baseline LLM
model (RawGPT ), achieving an 82.8% success rate (582/703) in pro-
ducing code that compiles and passes all tests, compared to just
8.7% (61/703) with RawGPT . Moreover, in comparison to IntelliJ’s
LLM-powered refactoring tool (EM-Assist), MANTRA exhibits a
50% improvement in generating Extract Method transformations. A
usability study involving 37 professional developers further shows
that refactorings performed by MANTRA are perceived to be as read-
able and reusable as human-written code, and in certain cases, even
more favorable. These results highlight the practical advantages
of MANTRA and emphasize the growing potential of LLM-based
systems in advancing the automation of software refactoring tasks

Large Language Models for Code (Code LLMs) are
increasingly employed in software development. However, studies
have recently shown that these models are vulnerable to backdoor
attacks: when a trigger (a specific input pattern) appears in the
input, the backdoor will be activated and cause the model to
generate malicious outputs desired by the attacker. Researchers
have designed various triggers and demonstrated the feasibility
of implanting backdoors by poisoning a fraction of the training
data (known as data poisoning). Some basic conclusions have
been made, such as backdoors becoming easier to implant when
attackers modify more training data. However, existing research
has not explored other factors influencing backdoor attacks on
Code LLMs, such as training batch size, epoch number, and the
broader design space for triggers, e.g., trigger length.
To bridge this gap, we use the code summarization task
as an example to perform a comprehensive empirical study
that systematically investigates the factors affecting backdoor
effectiveness and understands the extent of the threat posed by
backdoor attacks on Code LLMs. Three categories of factors
are considered: data, model, and inference, revealing findings
overlooked in previous studies for practitioners to mitigate
backdoor threats. For example, Code LLM developers can adopt
higher batch sizes with fewer epochs appropriately. Users of
code models can adjust inference parameters, such as using
a higher temperature or a larger top-k, appropriately. Future
backdoor defense can prioritize the inspection of rarer and
longer tokens, since they are more effective if they are indeed
triggers. Since these non-backdoor design factors can also greatly
sway attack performance, future backdoor studies should fully
report settings, control key factors, and systematically vary them
across configurations. What’s more, we find that the prevailing
consensus—that attacks are ineffective at extremely low poisoning
rates—is incorrect. The absolute number of poisoned samples
matters as well. Specifically, poisoning just 20 out of 454,451
samples (0.004% poisoning rate—far below the minimum setting
of 0.1% considered in prior Code LLM backdoor attack studies)
successfully implants backdoors! Moreover, the common defense
is incapable of removing even a single poisoned sample from
this poisoned dataset, highlighting the urgent need for defense
mechanisms against extremely low poisoning rate settings.

The field of software engineering and coding has undergone a significant transformation. The integration of large language models (LLMs), such as ChatGPT, into software development workflows is changing how developers at all skill levels approach coding tasks. Leveraging the capabilities of LLMs, developers can now implement functionalities, fix bugs, and address reviewers' comments more efficiently. However, prior research shows that the effectiveness of LLM-generated code is heavily influenced by the prompting strategy used. Furthermore, generating code at the class level is significantly more complex than at the method level, as it requires maintaining consistency across multiple methods and managing class state. Therefore, this study evaluates the impact of four prompting strategies (i.e., Zero-Shot, Few-Shot, Chain-of-Thought, and Chain-of-Thought-Few-Shot) on GPT and Llama3 in generating class-level code. It assesses the functional correctness and the quality characteristics of the generated code. To better understand how errors differ by prompting strategy, a qualitative analysis of the generated code is conducted for test cases that fail. The findings show that strategies incorporating more contextual guidance (Few-Shot, Chain-of-Thought, and Chain-of-Thought Few-Shot) outperform Zero-Shot prompting by up to 25% in functional correctness, 31% in BLEU-3 score, and 50% in ROUGE-L, while also producing code that is more readable and maintainable. The results also indicate that procedural logic and control flow errors are the most prominent, accounting for 31% of all errors. This study provides valuable insights to guide future research in developing techniques and tools that enhance the quality and reliability of LLM-generated code for complex software development tasks.

Excessively large classes that encapsulate multiple responsibilities are challenging to comprehend and maintain. Addressing this issue, several Extract Class refactoring tools have been proposed, employing a two-phase process: identifying suitable fields or methods for extraction, and implementing the mechanics of refactoring. These tools traditionally generate an intra-class dependency graph to analyze the class structure, applying hard-coded rules based on this graph to unearth refactoring opportunities. Yet, the graph-based approach predominantly illuminates direct, “one-to-one” relationship between pairwise entities. Such a perspective is restrictive as it overlooks the complex, “one-to-many” dependencies among multiple entities that are prevalent in real-world classes. This narrow focus can lead to refactoring suggestions that may diverge from developers’ actual needs, given their multifaceted nature. To bridge this gap, our paper leverages the concept of intra-class dependency hypergraph to model one-to-many dependency relationship and proposes a hypergraph learning-based approach to suggest Extract Class refactoring opportunities named HECS. For each target class, we first construct its intra-class dependency hypergraph and assign attributes to nodes with a pre-trained code model. All the attributed hypergraphs are fed into an enhanced hypergraph neural network for training. Utilizing this trained neural network alongside a large language model (LLM), we construct a refactoring suggestion system. We trained HECS on a large-scale dataset and evaluated it on two real-world datasets. The results show that demonstrates an increase of 38.5% in precision, 9.7% in recall, and 44.4% in f1-measure compared to 3 state-of-the-art refactoring tools including JDeodorant, SSECS, and LLMRefactor, which is more useful for 64% of participants. The results also unveil practical suggestions and new insights that benefit existing extract-related refactoring techniques.

Prompting LLMs with bug-related context (e.g., error messages, stack traces) improves automated program repair, but many bugs still remain unresolved. In real-world projects, developers often rely on broader repository and project-level context beyond the local code to resolve such bugs. In this paper, we investigate how automatically extracting and providing such knowledge can improve LLM-based program repair. We propose a layered knowledge injection framework that incrementally augments LLMs with structured context. It starts with the Bug Knowledge Layer, which includes information such as the buggy function and failing tests; expands to the Repository Knowledge Layer, which adds structural dependencies, related files, and commit history; and finally injects the Project Knowledge Layer, which incorporates relevant details from documentation and previously fixed bugs. We evaluate this framework on a dataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini), and analyze fix rates across six bug types. By progressively injecting knowledge across layers, our approach achieves a fix rate of 79% (250/314) using Llama 3.3, a significant improvement of 23% over previous work. All bug types show improvement with the addition of repository-level context, while only a subset benefit further from project-level knowledge, highlighting that different bug types require different levels of contextual information for effective repair. We also analyze the remaining unresolved bugs and find that more complex and structurally isolated bugs, such as Program Anomaly and GUI bugs, remain difficult even after injecting all available information. Our results show that layered context injection improves program repair and suggest the need for interactive and adaptive APR systems.

Flaky tests, characterized by inconsistent results across repeated executions, present significant challenges in software testing, especially during regression testing. Recently, there has been emerging research interest in non-idempotentoutcome (NIO) flaky tests-tests that pass on the initial run but fail on subsequent executions within the same environment. Despite progress in utilizing Large Language Models (LLMs) to address flaky tests, existing methods have not tackled NIO flaky tests. The limited context window of LLMs restricts their ability to incorporate relevant source code beyond the test method itself, often overlooking crucial information needed to address state pollution, which is the root cause of NIO flakiness. This paper introduces NIODebugger, the first framework to utilize an LLM-based agent to repair flaky tests. NIODebugger features a three-phase design: detection, exploration, and fixing. In the detection phase, dynamic analysis collects stack traces and custom test execution logs from multiple test runs, which helps in understanding accumulative state pollution. During the exploration phase, the LLM-based agent provides instructions for extracting relevant source code associated with test flakiness. In the fixing phase, NIODebugger repairs the tests using the information gathered from the previous phases. NIODebugger can be integrated with multiple LLMs, achieving patching success rates ranging from 11.63% to 58.72%. Its best-performing variant, NIODebugger-GPT-4, successfully generated correct patches for 101 out of 172 previously unknown NIO tests across 20 largescale open-source projects. We submitted pull requests for all generated patches; 58 have been merged, only 1 was rejected, and the remaining 42 are pending. The Java implementation of NIODebugger is provided as a Maven plugin accessible at https://github.com/kaiyaok2/NIOInspector.





































































































































